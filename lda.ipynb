{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all imports here\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_text_data = []\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def process_tweet(tweet):\n",
    "    # Split the tweet into words\n",
    "    words = str(tweet).split()\n",
    "    \n",
    "    # List comprehension to filter out stop words and short words\n",
    "    filtered_words = [\n",
    "        word for word in words \n",
    "        if word.lower() not in stop_words \n",
    "        and len(word) > 2 \n",
    "        and not word.startswith('http')\n",
    "        and word.isnumeric() == False\n",
    "    ]\n",
    "    \n",
    "    # Join the filtered words back into a string\n",
    "    processed = ' '.join(filtered_words)\n",
    "    \n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "base_directory = \"CrisisLexT26-v1.0\\CrisisLexT26\"\n",
    "\n",
    "text_data=pd.DataFrame()\n",
    "\n",
    "for folder_name in os.listdir(base_directory):\n",
    "    folder_path = os.path.join(base_directory, folder_name)\n",
    "    if os.path.isdir(folder_path):\n",
    "        json_file_path = os.path.join(folder_path, f\"{folder_name}-event_description.json\")\n",
    "        \n",
    "        #get event description. compare accuracy later\n",
    "        with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "            event_data = json.load(f)\n",
    "        \n",
    "        \n",
    "        tweets_csv_path = os.path.join(folder_path, f\"{folder_name}-tweets_labeled.csv\")\n",
    "                \n",
    "        df = pd.read_csv(tweets_csv_path)\n",
    "                \n",
    "        # Filter relevant tweets\n",
    "        df = df[df[' Informativeness'].isin(['Related - but not informative', 'Related and informative'])]\n",
    "\n",
    "        text_data = df[' Tweet Text'].apply(process_tweet).values\n",
    "\n",
    "        all_text_data.extend(text_data)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1\n",
      "0.312*\"West\" + 0.108*\"building\" + 0.048*\"say\" + 0.027*\"officials\" + 0.017*\"stop\" + 0.015*\"Lord\" + 0.014*\"across\" + 0.011*\"Sunday\" + 0.010*\"Friday\" + 0.010*\"Times\"\n",
      "Topic #2\n",
      "0.081*\"video\" + 0.050*\"deadly\" + 0.043*\"survivors\" + 0.031*\"North\" + 0.025*\"Latest\" + 0.017*\"weather\" + 0.017*\"YouTube\" + 0.016*\"LIVE\" + 0.014*\"Disaster\" + 0.013*\"Mayor\"\n",
      "Topic #3\n",
      "0.079*\"“\" + 0.056*\"AP\" + 0.043*\"”\" + 0.034*\"police\" + 0.029*\"man\" + 0.025*\"time\" + 0.024*\"VIDEO\" + 0.021*\"UPDATE\" + 0.021*\"morning\" + 0.016*\"hope\"\n",
      "Topic #4\n",
      "0.078*\"help\" + 0.039*\"need\" + 0.037*\"relief\" + 0.034*\"Death\" + 0.032*\"know\" + 0.029*\"may\" + 0.026*\"see\" + 0.025*\"please\" + 0.022*\"$\" + 0.019*\"still\"\n",
      "Topic #5\n",
      "0.212*\"?\" + 0.075*\"injured\" + 0.065*\"affected\" + 0.049*\"week\" + 0.031*\"tragedy\" + 0.024*\"City\" + 0.023*\"town\" + 0.017*\"people\" + 0.017*\"today\" + 0.015*\"back\"\n",
      "Topic #6\n",
      "0.060*\"Video\" + 0.033*\"cause\" + 0.027*\"Photo\" + 0.027*\"storm\" + 0.026*\"youtube\" + 0.022*\"Que\" + 0.020*\"--\" + 0.019*\"via\" + 0.016*\"change\" + 0.015*\"HuffingtonPost\"\n",
      "Topic #7\n",
      "0.057*\"death\" + 0.038*\"two\" + 0.037*\"Obama\" + 0.032*\"year\" + 0.025*\"live\" + 0.024*\"west\" + 0.023*\"homeless\" + 0.018*\"come\" + 0.017*\"coverage\" + 0.016*\"First\"\n",
      "Topic #8\n",
      "0.080*\"del\" + 0.052*\"que\" + 0.044*\":\" + 0.042*\"por\" + 0.040*\"@\" + 0.027*\"los\" + 0.027*\"con\" + 0.026*\"muertos\" + 0.026*\"world\" + 0.026*\"las\"\n",
      "Topic #9\n",
      "0.151*\"dead\" + 0.087*\"least\" + 0.068*\"people\" + 0.027*\"cnnbrk\" + 0.024*\"confirmed\" + 0.022*\"hundreds\" + 0.021*\"terrible\" + 0.020*\"Update\" + 0.016*\"needs\" + 0.013*\"NEW\"\n",
      "Topic #10\n",
      "0.041*\"]\" + 0.041*\"[\" + 0.038*\"bomb\" + 0.027*\"emergency\" + 0.022*\"coming\" + 0.021*\"latest\" + 0.019*\"mph\" + 0.015*\"seen\" + 0.014*\"let\" + 0.013*\"Harry_Styles\"\n",
      "Topic #11\n",
      "0.141*\".\" + 0.115*\";\" + 0.085*\"&\" + 0.070*\"amp\" + 0.043*\"@\" + 0.030*\":\" + 0.029*\"prayers\" + 0.027*\"families\" + 0.026*\"everyone\" + 0.025*\"floods\"\n",
      "Topic #12\n",
      "0.203*\"crash\" + 0.059*\"hit\" + 0.036*\"want\" + 0.028*\"support\" + 0.016*\"media\" + 0.014*\"close\" + 0.014*\"Live\" + 0.012*\"thing\" + 0.012*\"Full\" + 0.011*\"region\"\n",
      "Topic #13\n",
      "0.152*\"victims\" + 0.047*\"praying\" + 0.042*\"many\" + 0.041*\"far\" + 0.033*\"join\" + 0.029*\"Please\" + 0.021*\"People\" + 0.020*\"Let\" + 0.017*\"Another\" + 0.017*\"guys\"\n",
      "Topic #14\n",
      "0.043*\"update\" + 0.035*\"Prayers\" + 0.034*\"home\" + 0.028*\"gt\" + 0.022*\"n't\" + 0.022*\"work\" + 0.020*\"much\" + 0.018*\"would\" + 0.017*\"crashed\" + 0.017*\"Thank\"\n",
      "Topic #15\n",
      "0.439*\"!\" + 0.044*\"died\" + 0.027*\"last\" + 0.026*\"night\" + 0.020*\"RT\" + 0.019*\"get\" + 0.017*\"make\" + 0.015*\"found\" + 0.012*\"Help\" + 0.012*\"way\"\n",
      "Topic #16\n",
      "0.039*\"going\" + 0.030*\"Today\" + 0.025*\"víctimas\" + 0.022*\"got\" + 0.022*\"really\" + 0.022*\"give\" + 0.020*\"Pray\" + 0.019*\"más\" + 0.018*\"afectados\" + 0.015*\"closed\"\n",
      "Topic #17\n",
      "0.144*\"killed\" + 0.059*\"earthquake\" + 0.054*\"hits\" + 0.028*\"Reuters\" + 0.025*\"Costa\" + 0.023*\"Rica\" + 0.022*\"RT_com\" + 0.021*\"quake\" + 0.016*\"deaths\" + 0.011*\"major\"\n",
      "Topic #18\n",
      "0.070*\"toll\" + 0.050*\"blast\" + 0.049*\"News\" + 0.044*\"’\" + 0.026*\"...\" + 0.024*\"heart\" + 0.024*\"right\" + 0.023*\"ABC\" + 0.022*\":\" + 0.021*\"disaster\"\n",
      "Topic #19\n",
      "0.143*\"'\" + 0.100*\"..\" + 0.021*\"NEWS\" + 0.021*\"away\" + 0.020*\"collapsed\" + 0.020*\"footage\" + 0.017*\"visit\" + 0.016*\"never\" + 0.014*\"Minister\" + 0.014*\"believe\"\n",
      "Topic #20\n",
      "0.192*\"'s\" + 0.063*\"Colorado\" + 0.040*\"fire\" + 0.019*\"fires\" + 0.015*\"photo\" + 0.015*\"city\" + 0.015*\"homes\" + 0.015*\"also\" + 0.014*\"state\" + 0.014*\"could\"\n",
      "Topic #21\n",
      "0.098*\"like\" + 0.067*\"I\" + 0.066*\"CNN\" + 0.035*\"'m\" + 0.029*\"USA\" + 0.022*\"Hope\" + 0.019*\"looks\" + 0.019*\"Earth\" + 0.018*\"noticia\" + 0.017*\"Safety\"\n",
      "Topic #22\n",
      "0.113*\"haze\" + 0.064*\"BBCBreaking\" + 0.043*\"tonight\" + 0.036*\"bodies\" + 0.030*\"hearts\" + 0.022*\"'ve\" + 0.020*\"search\" + 0.020*\"new\" + 0.017*\"evacuated\" + 0.017*\"ever\"\n",
      "Topic #23\n",
      "0.330*\"#\" + 0.194*\"@\" + 0.183*\":\" + 0.103*\".\" + 0.050*\",\" + 0.050*\"Texas\" + 0.013*\"...\" + 0.011*\"via\" + 0.005*\"per\" + 0.004*\"news\"\n",
      "Topic #24\n",
      "0.065*\"flooding\" + 0.030*\"family\" + 0.024*\"hear\" + 0.023*\"call\" + 0.023*\"following\" + 0.022*\"Thoughts\" + 0.022*\"keep\" + 0.021*\"caused\" + 0.021*\"les\" + 0.020*\"rain\"\n",
      "Topic #25\n",
      "0.242*\":\" + 0.177*\",\" + 0.159*\"@\" + 0.079*\"...\" + 0.072*\".\" + 0.022*\"people\" + 0.014*\"New\" + 0.012*\"helicopter\" + 0.010*\"says\" + 0.010*\"BREAKING\"\n",
      "Topic #26\n",
      "0.211*\"(\" + 0.198*\")\" + 0.028*\"aid\" + 0.016*\"shelter\" + 0.014*\"even\" + 0.014*\"-\" + 0.012*\"hospital\" + 0.010*\"ago\" + 0.008*\"left\" + 0.007*\"Así\"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models import LdaModel\n",
    "      \n",
    "\n",
    "tokens = [word_tokenize(doc) for doc in all_text_data]\n",
    "tokens = [[word for word in doc if word not in stop_words] for doc in tokens]\n",
    "\n",
    "# Create Gensim dictionary\n",
    "dictionary = Dictionary(tokens)\n",
    "\n",
    "# Convert your tokenized texts to a bag-of-words format\n",
    "corpus = [dictionary.doc2bow(text) for text in tokens]\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=5, max_df=0.9, stop_words='english', ngram_range=(1, 2))\n",
    "text_tfidf = vectorizer.fit_transform(all_text_data)\n",
    "\n",
    "\n",
    "lda_gensim = LdaModel(corpus, num_topics=26, id2word=dictionary, passes=15)\n",
    "\n",
    "lda = LatentDirichletAllocation(random_state=42)\n",
    "lda.fit(text_tfidf)\n",
    "\n",
    "# Print topics\n",
    "for index, topic in lda_gensim.print_topics(-1):\n",
    "    print(f\"Topic #{index + 1}\")\n",
    "    print(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "derailment train derailment metro north metro north train nyc nyc train dead north train\n",
      "Topic #2:\n",
      "costa costa rica rica terremoto costa tsunami terremoto alerta alerta tsunami quake hits costa\n",
      "Topic #3:\n",
      "bangladesh collapse building building collapse bangladesh building factory factory collapse bangladesh factory garment collapse bangladesh\n",
      "Topic #4:\n",
      "brazil nightclub nightclub brazil praying affected sad hear sad brazil tragedy tragedy praying hear nightclub ddlovato\n",
      "Topic #5:\n",
      "helicopter glasgow helicopter crash crash pub glasgow helicopter clutha police crash glasgow police helicopter\n",
      "Topic #6:\n",
      "typhoon haiyan philippines typhoon haiyan bopha typhoon bopha pablo typhoon pablo super super typhoon\n",
      "Topic #7:\n",
      "tren santiago accidente compostela santiago compostela accidente tren españa del tren santiago tren españa\n",
      "Topic #8:\n",
      "colorado flooding springs colorado springs floods colorado floods wildfire colorado flooding wildfires colorado wildfire\n",
      "Topic #9:\n",
      "texas explosion plant fertilizer west fertilizer plant plant explosion west texas waco explosion texas\n",
      "Topic #10:\n",
      "meteor russia russian russian meteor meteor russia hit russianmeteor shower meteor shower meteor hit\n",
      "Topic #11:\n",
      "sardegna alluvione alluvione sardegna allertameteosar che olbia non forzasardegna allertameteosar sardegna della\n",
      "Topic #12:\n",
      "amuay refinería refinería amuay que explosión los explosión refinería venezuela por las\n",
      "Topic #13:\n",
      "lax shooting airport suspect lax shooting tsa los angeles angeles los shooter\n",
      "Topic #14:\n",
      "sghaze psi singapore haze update stcom sghaze update stcom sghaze neasg update psi\n",
      "Topic #15:\n",
      "earthquake philippines magnitude magnitude earthquake hits earthquake hits quake guatemala italy coast\n",
      "Topic #16:\n",
      "boston marathon boston marathon bombing boston bombing marathon bombing explosions suspect bomb bombing suspect\n",
      "Topic #17:\n",
      "lac mégantic lac mégantic megantic lac megantic les pour quebec des lacmegantic\n",
      "Topic #18:\n",
      "rescueph help reliefph need pls amp victims maringph donate relief\n",
      "Topic #19:\n",
      "метеорит челябинск метеорит челябинск челябинске что челябинске метеорит ruredaktor самом деле деле что самом\n",
      "Topic #20:\n",
      "terremoto guatemala sismo por muertos terremoto guatemala por terremoto italia del magnitud\n",
      "Topic #21:\n",
      "boate kiss boate kiss que incêndio santamaria santa santa maria maria incêndio boate\n",
      "Topic #22:\n",
      "train new york new york train crash crash spain york train derailed spain train\n",
      "Topic #23:\n",
      "prayforvisayas bohol cebu pray god prayforbohol prayforcebu cebu bohol safe let\n",
      "Topic #24:\n",
      "prayforboston prayfortexas world prayfortexas prayforboston people prayforboston prayfortexas really prayforwest sick sad\n",
      "Topic #25:\n",
      "toll death death toll rises toll rises collapse death toll bangladesh passes quake brazil nightclub\n",
      "Topic #26:\n",
      "safe yycflood nswfires fires stay australia amp stay safe prayers thoughts\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "n_topics = 26\n",
    "\n",
    "# Apply NMF\n",
    "nmf = NMF(n_components=n_topics, random_state=42, l1_ratio=.5).fit(text_tfidf)\n",
    "\n",
    "# Print the topics\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[:-10 - 1:-1]]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score for LDA:  0.5055515340333058\n",
      "\n",
      "Coherence Score for NMF:  0.5557482579334588\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Extract top words for each NMF topic\n",
    "top_words = []\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic in nmf.components_:\n",
    "    top_word_indices = topic.argsort()[-10:][::-1]  # You can change 10 to any other number\n",
    "    top_words.append([feature_names[i] for i in top_word_indices])\n",
    "\n",
    "# Compute coherence for NMF topics\n",
    "texts = tokens\n",
    "nmf_topics = [[dictionary.token2id[word] for word in topic if word in dictionary.token2id] for topic in top_words]\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_gensim, texts=tokens, dictionary=dictionary, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score for LDA: ', coherence_lda)\n",
    "\n",
    "coherence_model_nmf = CoherenceModel(topics=nmf_topics, texts=tokens, dictionary=dictionary, coherence='c_v')\n",
    "coherence_nmf = coherence_model_nmf.get_coherence()\n",
    "print('\\nCoherence Score for NMF: ', coherence_nmf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
